# CorrupciÃ³n Data Pipeline

Automated data scraping, ETL, and loading pipeline for corruption case data.

## ğŸ“‹ Prerequisites

- Docker and Docker Compose installed
- At least 2GB of free disk space
- Internet connection for scraping

## ğŸš€ Quick Start

### 1. Initial Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd corrupcion-pipeline

# Create required directories
mkdir -p data logs initdb

# Copy your init.sql to initdb/
cp /path/to/your/init.sql initdb/

# Create environment file (optional)
cp .env.example .env
```

### 2. Start the Services

```bash
# Start database and scheduler
docker-compose up -d postgres scheduler

# Wait for database to be ready (about 10 seconds)
docker-compose logs -f postgres
# Press Ctrl+C when you see "database system is ready to accept connections"
```

### 3. Run Pipeline Manually (First Time)

```bash
# Make the script executable
chmod +x run_pipeline.sh

# Run the entire pipeline
./run_pipeline.sh
```

Or run each step individually:

```bash
# Step 1: Run scrapers
docker-compose run --rm scraper

# Step 2: Run ETL
docker-compose run --rm etl

# Step 3: Load data
docker-compose run --rm loader
```

## ğŸ“… Automated Weekly Runs

The pipeline runs automatically every **Sunday at 2:00 AM** via the scheduler service.

The schedule:
- **2:00 AM** - Scrapers run (generates CSVs)
- **4:00 AM** - ETL processes the CSVs (2 hours after scrapers)
- **4:15 AM** - Loader inserts data into database

To modify the schedule, edit the `ofelia.job-run.*.schedule` labels in `docker-compose.yml`.

### Cron Schedule Format
```
0 0 2 * * 0  = Every Sunday at 2:00 AM
â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€ Day of week (0-6, 0=Sunday)
â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€ Month (1-12)
â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€ Day of month (1-31)
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hour (0-23)
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Minute (0-59)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Second (0-59)
```

## ğŸ“Š Monitoring

### Check Scheduler Status
```bash
docker-compose logs -f scheduler
```

### Check Last Run Logs
```bash
# Scraper logs
ls -lht logs/scraper_*.log | head -1

# ETL logs
docker-compose logs etl

# Loader logs
docker-compose logs loader
```

### View Generated CSVs
```bash
ls -lh data/
```

### Query Database
```bash
# Connect to database
docker-compose exec postgres psql -U admin -d corrupcion_db

# Check row counts
SELECT tablename, n_live_tup 
FROM pg_stat_user_tables 
ORDER BY n_live_tup DESC;
```

## ğŸ› ï¸ Management Commands

### Stop All Services
```bash
docker-compose down
```

### Stop and Remove All Data
```bash
docker-compose down -v
# WARNING: This deletes the database!
```

### Restart a Specific Service
```bash
docker-compose restart scheduler
```

### View All Logs
```bash
docker-compose logs -f
```

### Rebuild After Code Changes
```bash
docker-compose build scraper etl loader
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ docker-compose.yml          # Main orchestration file
â”œâ”€â”€ Dockerfile.scraper          # Scraper container
â”œâ”€â”€ Dockerfile.etl              # ETL container (same for loader)
â”œâ”€â”€ run_scrapers.py             # Sequential scraper runner
â”œâ”€â”€ run_pipeline.sh             # Manual pipeline runner
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ scraper_entramite.py       # Scraper script 1
â”œâ”€â”€ scraper_completas.py        # Scraper script 2
â”œâ”€â”€ scraper_jueces.py          # Scraper script 3
â”œâ”€â”€ transform_expedientes.py          # ETL processor
â”œâ”€â”€ cargar_etl.py               # Database loader
â”œâ”€â”€ data/                       # Generated CSVs (gitignored)
â”œâ”€â”€ logs/                       # Application logs (gitignored)
â””â”€â”€ initdb/
    â””â”€â”€ init.sql                # Database initialization
```

## ğŸ”§ Configuration

### Database Connection

Set via environment variables in `docker-compose.yml`:
- `DB_HOST=postgres`
- `DB_PORT=5432`
- `DB_NAME=corrupcion_db`
- `DB_USER=admin`
- `DB_PASSWORD=td8corrupcion`

### Changing Schedule

Edit `docker-compose.yml` scheduler labels:
```yaml
labels:
  # Run every day at 3 AM instead
  ofelia.job-run.scraper.schedule: "0 0 3 * * *"
```

## ğŸ› Troubleshooting

### Database won't start
```bash
# Check logs
docker-compose logs postgres

# Reset database
docker-compose down -v
docker-compose up -d postgres
```

### Scraper fails
```bash
# Check logs
docker-compose logs scraper
cat logs/scraper_*.log | tail -50

# Run in interactive mode for debugging
docker-compose run --rm scraper /bin/bash
```

### ETL fails
```bash
# Check if CSVs exist
ls -lh data/

# Check ETL logs
docker-compose logs etl
```

### Scheduler not running
```bash
# Check scheduler logs
docker-compose logs scheduler

# Restart scheduler
docker-compose restart scheduler
```

## ğŸ”’ Security Notes

- **Change default passwords** in production!
- Don't commit `.env` files with credentials
- Restrict database port (5432) in production
- Consider using Docker secrets for sensitive data

## ğŸ¤ For Other Users

To run this pipeline:

1. Install Docker and Docker Compose
2. Clone this repository
3. Place `init.sql` in `initdb/` folder
4. Run `docker-compose up -d`
5. Run `./run_pipeline.sh` for first-time setup
6. Pipeline will run automatically every Sunday

That's it! No Python environment setup, no dependency issues.